#!/usr/bin/env python3
"""
Script de d√©marrage simple pour APOCALIPSSI
Lance automatiquement le backend et le frontend avec configuration Ollama
"""

import os
import sys
import subprocess
import time
import platform
import requests
from pathlib import Path

def print_step(message):
    """Afficher une √©tape avec formatage"""
    print(f"\n{'='*50}")
    print(f"üîÑ {message}")
    print(f"{'='*50}")

def print_success(message):
    """Afficher un message de succ√®s"""
    print(f"‚úÖ {message}")

def print_error(message):
    """Afficher un message d'erreur"""
    print(f"‚ùå {message}")

def print_info(message):
    """Afficher un message d'information"""
    print(f"‚ÑπÔ∏è  {message}")

def run_command(command, cwd=None, shell=True, capture_output=False, timeout=300):
    """Ex√©cuter une commande de mani√®re simple avec timeout"""
    try:
        if capture_output:
            result = subprocess.run(command, cwd=cwd, shell=shell, check=True, 
                                  capture_output=True, text=True, timeout=timeout)
            return True, result.stdout
        else:
            subprocess.run(command, cwd=cwd, shell=shell, check=True, timeout=timeout)
            return True, None
    except subprocess.CalledProcessError as e:
        return False, str(e)
    except subprocess.TimeoutExpired:
        return False, "Timeout - commande trop longue"

def check_ollama_installed():
    """V√©rifier si Ollama est d√©j√† install√©"""
    # Essayer d'abord avec le PATH normal
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True, timeout=10)
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError):
        pass
    
    # Si pas trouv√©, essayer les chemins Windows courants
    import platform
    if platform.system().lower() == "windows":
        possible_paths = [
            r"C:\Program Files\Ollama\ollama.exe",
            r"C:\Users\%USERNAME%\AppData\Local\Programs\Ollama\ollama.exe",
            r"C:\Users\%USERNAME%\AppData\Local\Ollama\ollama.exe",
            r"C:\Users\%USERNAME%\AppData\Local\Programs\Ollama\ollama app.exe"
        ]
        
        for path in possible_paths:
            expanded_path = os.path.expandvars(path)
            if os.path.exists(expanded_path):
                try:
                    result = subprocess.run([expanded_path, '--version'], 
                                          capture_output=True, text=True, timeout=10)
                    if result.returncode == 0:
                        # Ajouter au PATH pour cette session
                        ollama_dir = os.path.dirname(expanded_path)
                        os.environ['PATH'] = ollama_dir + os.pathsep + os.environ['PATH']
                        print_success(f"Ollama trouv√© dans: {ollama_dir}")
                        return True
                except Exception as e:
                    print_info(f"Test du chemin {expanded_path}: {e}")
                    pass
    
    return False

def install_ollama():
    """Installer Ollama selon le syst√®me d'exploitation"""
    system = platform.system().lower()
    
    print_step("Installation d'Ollama")
    
    if system == "windows":
        print_info("Installation sur Windows...")
        # Essayer winget d'abord
        print_info("Tentative d'installation via winget (cela peut prendre quelques minutes)...")
        success, output = run_command("winget install Ollama.Ollama --accept-source-agreements --accept-package-agreements", capture_output=True)
        if success:
            print_success("Ollama install√© via winget")
            return True
        else:
            print_info("Installation winget √©chou√©e ou prise trop de temps")
            print_info("T√©l√©chargement manuel requis")
            print_info("1. Visitez: https://ollama.ai/download")
            print_info("2. T√©l√©chargez et installez Ollama pour Windows")
            print_info("3. Red√©marrez votre terminal apr√®s l'installation")
            input("Appuyez sur Entr√©e une fois Ollama install√©...")
            return check_ollama_installed()
        
    elif system == "darwin":  # macOS
        print_info("Installation sur macOS...")
        success, _ = run_command("brew install ollama", capture_output=True)
        if success:
            print_success("Ollama install√© via Homebrew")
            return True
        else:
            print_error("Erreur lors de l'installation via Homebrew")
            print_info("T√©l√©chargez depuis: https://ollama.ai/download")
            return False
            
    elif system == "linux":
        print_info("Installation sur Linux...")
        success, _ = run_command("curl -fsSL https://ollama.ai/install.sh | sh", 
                                capture_output=True)
        if success:
            print_success("Ollama install√© via le script officiel")
            return True
        else:
            print_error("Erreur lors de l'installation")
            print_info("Consultez: https://ollama.ai/download")
            return False
    
    return False

def start_ollama_service():
    """D√©marrer le service Ollama"""
    print_step("D√©marrage du service Ollama")
    
    try:
        # D√©marrer Ollama en arri√®re-plan
        subprocess.Popen(['ollama', 'serve'], 
                        stdout=subprocess.DEVNULL, 
                        stderr=subprocess.DEVNULL)
        
        # Attendre que le service soit pr√™t
        for i in range(30):  # Attendre max 30 secondes
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    print_success("Service Ollama d√©marr√© avec succ√®s")
                    return True
            except:
                pass
            time.sleep(1)
            print(f"‚è≥ Attente du service Ollama... ({i+1}/30)")
        
        print_error("Timeout: Le service Ollama n'a pas d√©marr√©")
        return False
        
    except Exception as e:
        print_error(f"Erreur lors du d√©marrage: {e}")
        return False

def download_ollama_model(model_name="llama3.2:3b"):
    """T√©l√©charger un mod√®le Ollama"""
    print_step(f"T√©l√©chargement du mod√®le {model_name}")
    
    try:
        print_info("T√©l√©chargement en cours... (cela peut prendre plusieurs minutes)")
        success, output = run_command(f"ollama pull {model_name}", capture_output=True)
        
        if success:
            print_success(f"Mod√®le {model_name} t√©l√©charg√© avec succ√®s")
            return True
        else:
            print_error(f"Erreur lors du t√©l√©chargement: {output}")
            return False
            
    except Exception as e:
        print_error(f"Erreur: {e}")
        return False

def check_env():
    """V√©rifier et cr√©er le fichier .env si n√©cessaire"""
    env_file = Path(".env")
    if not env_file.exists():
        print_step("Cr√©ation du fichier .env")
        env_content = """# Configuration APOCALIPSSI

# Base de donn√©es MongoDB
MONGO_URI=mongodb://localhost:27017

# API Groq (optionnel - fallback)
GROQ_API_KEY=your_groq_api_key_here

# Configuration Ollama (mod√®le local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
USE_LOCAL_MODEL=true

# Configuration JWT
JWT_SECRET_KEY=your_jwt_secret_key_here

# Configuration Flask (optionnel)
FLASK_ENV=development
FLASK_DEBUG=True
"""
        with open(env_file, "w", encoding="utf-8") as f:
            f.write(env_content)
        print_success("Fichier .env cr√©√© avec configuration Ollama")
    else:
        # V√©rifier si les configurations Ollama sont pr√©sentes
        content = env_file.read_text()
        ollama_configs = [
            "OLLAMA_BASE_URL=http://localhost:11434",
            "OLLAMA_MODEL=llama3.2:3b", 
            "USE_LOCAL_MODEL=true"
        ]
        
        missing_configs = []
        for config in ollama_configs:
            if config.split('=')[0] not in content:
                missing_configs.append(config)
        
        if missing_configs:
            print_info("Mise √† jour du fichier .env avec les configurations Ollama")
            for config in missing_configs:
                content += f"\n{config}"
            env_file.write_text(content)
            print_success("Fichier .env mis √† jour")

def setup_ollama():
    """Configuration compl√®te d'Ollama"""
    print_step("Configuration d'Ollama")
    
    # V√©rifier si Ollama est install√©
    if not check_ollama_installed():
        print_info("Ollama n'est pas install√© ou pas dans le PATH")
        print_info("Tentative d'installation...")
        if not install_ollama():
            print_error("Installation d'Ollama √©chou√©e")
            print_info("L'application continuera avec l'API externe si configur√©e")
            return False
        
        # Apr√®s installation, v√©rifier √† nouveau
        if not check_ollama_installed():
            print_error("Ollama install√© mais pas d√©tect√©")
            print_info("Red√©marrez votre terminal et relancez le script")
            print_info("Ou ajoutez manuellement Ollama au PATH")
            print_info("L'application continuera avec l'API externe si configur√©e")
            return False
    else:
        print_success("Ollama est d√©j√† install√© et d√©tect√©")
    
    # D√©marrer le service
    if not start_ollama_service():
        print_error("Impossible de d√©marrer le service Ollama")
        print_info("L'application continuera avec l'API externe si configur√©e")
        return False
    
    # T√©l√©charger le mod√®le
    model_name = "llama3.2:3b"
    if not download_ollama_model(model_name):
        print_error("T√©l√©chargement du mod√®le √©chou√©")
        print_info("L'application continuera avec l'API externe si configur√©e")
        return False
    
    print_success("Configuration Ollama termin√©e avec succ√®s")
    return True

def main():
    """Fonction principale simplifi√©e"""
    print("üöÄ APOCALIPSSI - D√©marrage automatique avec Ollama")
    print("=" * 60)
    
    # V√©rifier Python
    if sys.version_info < (3, 8):
        print_error("Python 3.8+ requis")
        sys.exit(1)
    print_success(f"Python {sys.version.split()[0]} d√©tect√©")
    
    # V√©rifier et cr√©er .env avec configuration Ollama
    check_env()
    
    # Configuration d'Ollama
    ollama_success = setup_ollama()
    
    # Installer les d√©pendances backend
    print_step("Installation des d√©pendances backend")
    if not run_command("pip install -r backend/requirements.txt")[0]:
        print_error("Erreur lors de l'installation des d√©pendances backend")
        sys.exit(1)
    print_success("D√©pendances backend install√©es")
    
    # Installer spaCy pour l'anonymisation PII
    print_step("Configuration de l'anonymisation PII")
    try:
        import spacy
        print_success("spaCy d√©j√† install√©")
    except ImportError:
        print_info("Installation de spaCy pour l'anonymisation PII...")
        if not run_command("pip install spacy>=3.7.0")[0]:
            print_error("Erreur lors de l'installation de spaCy")
            print_info("L'anonymisation PII ne sera pas disponible")
        else:
            print_success("spaCy install√© pour l'anonymisation PII")
    
    # Installer les d√©pendances frontend
    print_step("Installation des d√©pendances frontend")
    if not run_command("npm install", cwd="frontend")[0]:
        print_error("Erreur lors de l'installation des d√©pendances frontend")
        print_info("Assurez-vous que Node.js est install√©: https://nodejs.org/")
        sys.exit(1)
    print_success("D√©pendances frontend install√©es")
    
    # D√©marrer le backend
    print_step("D√©marrage du backend")
    backend_process = subprocess.Popen([sys.executable, "app.py"], cwd="backend")
    print_success("Backend d√©marr√© sur http://localhost:5000")
    
    # Attendre un peu
    time.sleep(2)
    
    # D√©marrer le frontend
    print_step("D√©marrage du frontend")
    frontend_process = subprocess.Popen("npm run dev", cwd="frontend", shell=True)
    print_success("Frontend d√©marr√© sur http://localhost:5173")
    
    print("\n" + "=" * 60)
    print("üéâ Application APOCALIPSSI d√©marr√©e avec succ√®s !")
    print("\nüì± Acc√®s:")
    print("   Frontend: http://localhost:5173")
    print("   Backend:  http://localhost:5000")
    
    if ollama_success:
        print("\nü§ñ Mod√®le LLM local Ollama configur√© et pr√™t")
        print("   L'application utilise maintenant un mod√®le local pour l'analyse")
    else:
        print("\n‚ö†Ô∏è  Ollama non configur√©")
        print("   L'application utilisera l'API externe si configur√©e")
    
    print("\nüí° Pour arr√™ter l'application, appuyez sur Ctrl+C")
    
    try:
        # Attendre que l'un des processus se termine
        while backend_process.poll() is None and frontend_process.poll() is None:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nüõë Arr√™t de l'application...")
        backend_process.terminate()
        frontend_process.terminate()
        print_success("Application arr√™t√©e")

if __name__ == "__main__":
    main() 